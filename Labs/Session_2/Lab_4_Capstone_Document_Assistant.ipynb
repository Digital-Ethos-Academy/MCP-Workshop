{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Capstone - Document Assistant MCP Server\n",
    "\n",
    "**Objective:** Build a complete MCP server that serves as a Document Assistant, combining tools, resources, and prompts.\n",
    "\n",
    "**Duration:** ~45 minutes\n",
    "\n",
    "**What You'll Build:**\n",
    "- A fully-featured MCP server for document management\n",
    "- Search tools to find documents by content or metadata\n",
    "- Resources to expose document contents\n",
    "- Prompts for common document analysis tasks\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Labs 1-3\n",
    "- Understanding of MCP tools, resources, and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Project Overview\n",
    "\n",
    "The Document Assistant will help users:\n",
    "1. **Search** documents by keyword or metadata\n",
    "2. **Read** document contents via resources\n",
    "3. **Analyze** documents using pre-built prompts\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Document Assistant MCP Server\n",
    "├── Tools\n",
    "│   ├── search_documents(query) - Search by content\n",
    "│   ├── list_documents() - List all documents\n",
    "│   └── get_document_stats() - Get statistics\n",
    "├── Resources\n",
    "│   ├── doc://{doc_id} - Individual document content\n",
    "│   └── docs://catalog - Full document catalog\n",
    "└── Prompts\n",
    "    ├── summarize - Summarize a document\n",
    "    ├── compare - Compare two documents\n",
    "    └── extract-insights - Extract key insights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up the Project\n",
    "\n",
    "Let's create the project structure and sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create project directory\n",
    "os.makedirs(\"document_assistant\", exist_ok=True)\n",
    "os.makedirs(\"document_assistant/documents\", exist_ok=True)\n",
    "\n",
    "print(\"Project structure created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for testing\n",
    "\n",
    "documents = {\n",
    "    \"project_proposal.md\": '''# Project Proposal: AI Integration\n",
    "\n",
    "## Executive Summary\n",
    "This proposal outlines a plan to integrate AI capabilities into our existing product suite.\n",
    "\n",
    "## Objectives\n",
    "1. Improve customer experience through intelligent recommendations\n",
    "2. Automate repetitive tasks to increase efficiency\n",
    "3. Provide predictive analytics for business decisions\n",
    "\n",
    "## Timeline\n",
    "- Phase 1: Research and Planning (Q1)\n",
    "- Phase 2: Development (Q2-Q3)\n",
    "- Phase 3: Testing and Deployment (Q4)\n",
    "\n",
    "## Budget\n",
    "Estimated total: $500,000\n",
    "''',\n",
    "    \n",
    "    \"technical_spec.md\": '''# Technical Specification: API Gateway\n",
    "\n",
    "## Overview\n",
    "The API Gateway will serve as the central entry point for all microservices.\n",
    "\n",
    "## Requirements\n",
    "- Handle 10,000 requests per second\n",
    "- Support OAuth 2.0 authentication\n",
    "- Implement rate limiting\n",
    "- Provide request/response logging\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "Client -> Load Balancer -> API Gateway -> Microservices\n",
    "```\n",
    "\n",
    "## Technologies\n",
    "- Language: Go\n",
    "- Framework: Gin\n",
    "- Database: PostgreSQL\n",
    "- Cache: Redis\n",
    "''',\n",
    "    \n",
    "    \"meeting_notes.md\": '''# Meeting Notes: Q4 Planning\n",
    "\n",
    "**Date:** October 15, 2024\n",
    "**Attendees:** Alice, Bob, Carol, David\n",
    "\n",
    "## Discussion Points\n",
    "\n",
    "### Budget Review\n",
    "- Current spending is 5% under budget\n",
    "- Proposal to reallocate funds to AI initiative\n",
    "\n",
    "### Hiring Plans\n",
    "- Need 3 senior engineers\n",
    "- 1 product manager opening\n",
    "\n",
    "### Action Items\n",
    "- [ ] Alice: Finalize budget proposal\n",
    "- [ ] Bob: Post job listings\n",
    "- [ ] Carol: Schedule follow-up meeting\n",
    "\n",
    "## Next Meeting\n",
    "October 22, 2024 at 2:00 PM\n",
    "''',\n",
    "    \n",
    "    \"security_policy.md\": '''# Security Policy\n",
    "\n",
    "## Purpose\n",
    "This document outlines security requirements for all company systems.\n",
    "\n",
    "## Password Requirements\n",
    "- Minimum 12 characters\n",
    "- Must include uppercase, lowercase, numbers, and symbols\n",
    "- Changed every 90 days\n",
    "\n",
    "## Data Classification\n",
    "1. **Public**: Marketing materials, public documentation\n",
    "2. **Internal**: Employee communications, internal processes\n",
    "3. **Confidential**: Customer data, financial records\n",
    "4. **Restricted**: Security credentials, encryption keys\n",
    "\n",
    "## Incident Response\n",
    "1. Identify and contain the threat\n",
    "2. Notify security team immediately\n",
    "3. Document all actions taken\n",
    "4. Conduct post-incident review\n",
    "'''\n",
    "}\n",
    "\n",
    "# Save documents\n",
    "for filename, content in documents.items():\n",
    "    filepath = f\"document_assistant/documents/{filename}\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Created: {filepath}\")\n",
    "\n",
    "print(f\"\\nCreated {len(documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building the Server - Step by Step\n",
    "\n",
    "Let's build the Document Assistant incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Basic Server Structure\n",
    "\n",
    "First, create the server with document loading capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_code = '''\n",
    "\"\"\"Document Assistant MCP Server - Step 1: Basic Structure\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_DIR = Path(__file__).parent / \"documents\"\n",
    "\n",
    "# Create the MCP server\n",
    "mcp = FastMCP(\n",
    "    \"Document Assistant\",\n",
    "    description=\"An MCP server for document management and analysis\"\n",
    ")\n",
    "\n",
    "# Document storage (loaded at startup)\n",
    "document_cache = {}\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"Load all documents from the documents directory.\"\"\"\n",
    "    global document_cache\n",
    "    document_cache = {}\n",
    "    \n",
    "    if not DOCUMENTS_DIR.exists():\n",
    "        return\n",
    "    \n",
    "    for filepath in DOCUMENTS_DIR.glob(\"*.md\"):\n",
    "        doc_id = filepath.stem  # filename without extension\n",
    "        stat = filepath.stat()\n",
    "        \n",
    "        document_cache[doc_id] = {\n",
    "            \"id\": doc_id,\n",
    "            \"filename\": filepath.name,\n",
    "            \"path\": str(filepath),\n",
    "            \"content\": filepath.read_text(),\n",
    "            \"size\": stat.st_size,\n",
    "            \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
    "        }\n",
    "    \n",
    "    print(f\"Loaded {len(document_cache)} documents\")\n",
    "\n",
    "# Load documents at startup\n",
    "load_documents()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "'''\n",
    "\n",
    "with open(\"document_assistant/server_step1.py\", \"w\") as f:\n",
    "    f.write(step1_code)\n",
    "\n",
    "print(\"Step 1 complete: Basic server structure created\")\n",
    "print(\"\\nThis creates:\")\n",
    "print(\"- MCP server instance\")\n",
    "print(\"- Document loading function\")\n",
    "print(\"- Document cache for fast access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add Search Tools\n",
    "\n",
    "Add tools for searching and listing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_code = '''\n",
    "\"\"\"Document Assistant MCP Server - Step 2: Search Tools\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_DIR = Path(__file__).parent / \"documents\"\n",
    "\n",
    "# Create the MCP server\n",
    "mcp = FastMCP(\n",
    "    \"Document Assistant\",\n",
    "    description=\"An MCP server for document management and analysis\"\n",
    ")\n",
    "\n",
    "# Document storage\n",
    "document_cache = {}\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"Load all documents from the documents directory.\"\"\"\n",
    "    global document_cache\n",
    "    document_cache = {}\n",
    "    \n",
    "    if not DOCUMENTS_DIR.exists():\n",
    "        return\n",
    "    \n",
    "    for filepath in DOCUMENTS_DIR.glob(\"*.md\"):\n",
    "        doc_id = filepath.stem\n",
    "        stat = filepath.stat()\n",
    "        \n",
    "        document_cache[doc_id] = {\n",
    "            \"id\": doc_id,\n",
    "            \"filename\": filepath.name,\n",
    "            \"path\": str(filepath),\n",
    "            \"content\": filepath.read_text(),\n",
    "            \"size\": stat.st_size,\n",
    "            \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
    "        }\n",
    "\n",
    "load_documents()\n",
    "\n",
    "# ============ TOOLS ============\n",
    "\n",
    "@mcp.tool()\n",
    "def list_documents() -> str:\n",
    "    \"\"\"List all available documents with their metadata.\n",
    "    \n",
    "    Returns:\n",
    "        A formatted list of all documents with ID, filename, size, and modification date.\n",
    "    \"\"\"\n",
    "    if not document_cache:\n",
    "        return \"No documents found.\"\n",
    "    \n",
    "    lines = [\"Available Documents:\", \"=\" * 50]\n",
    "    \n",
    "    for doc_id, doc in sorted(document_cache.items()):\n",
    "        lines.append(f\"\\nID: {doc_id}\")\n",
    "        lines.append(f\"  Filename: {doc['filename']}\")\n",
    "        lines.append(f\"  Size: {doc['size']} bytes\")\n",
    "        lines.append(f\"  Modified: {doc['modified']}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search_documents(query: str, case_sensitive: bool = False) -> str:\n",
    "    \"\"\"Search for documents containing the specified text.\n",
    "    \n",
    "    Args:\n",
    "        query: The text to search for in document contents\n",
    "        case_sensitive: Whether the search should be case-sensitive (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        A list of matching documents with excerpts showing where the query was found.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Error: Please provide a search query.\"\n",
    "    \n",
    "    results = []\n",
    "    flags = 0 if case_sensitive else re.IGNORECASE\n",
    "    \n",
    "    for doc_id, doc in document_cache.items():\n",
    "        content = doc[\"content\"]\n",
    "        matches = list(re.finditer(re.escape(query), content, flags))\n",
    "        \n",
    "        if matches:\n",
    "            # Get excerpt around first match\n",
    "            first_match = matches[0]\n",
    "            start = max(0, first_match.start() - 50)\n",
    "            end = min(len(content), first_match.end() + 50)\n",
    "            excerpt = content[start:end].replace(\"\\n\", \" \")\n",
    "            \n",
    "            if start > 0:\n",
    "                excerpt = \"...\" + excerpt\n",
    "            if end < len(content):\n",
    "                excerpt = excerpt + \"...\"\n",
    "            \n",
    "            results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": doc[\"filename\"],\n",
    "                \"match_count\": len(matches),\n",
    "                \"excerpt\": excerpt\n",
    "            })\n",
    "    \n",
    "    if not results:\n",
    "        return f\"No documents found containing '{query}'.\"\n",
    "    \n",
    "    lines = [f\"Found {len(results)} document(s) matching '{query}':\", \"=\" * 50]\n",
    "    \n",
    "    for result in results:\n",
    "        lines.append(f\"\\nDocument: {result['doc_id']}\")\n",
    "        lines.append(f\"  Matches: {result['match_count']}\")\n",
    "        lines.append(f\"  Excerpt: {result['excerpt']}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_document_stats() -> str:\n",
    "    \"\"\"Get statistics about the document collection.\n",
    "    \n",
    "    Returns:\n",
    "        Statistics including total documents, total size, and average document size.\n",
    "    \"\"\"\n",
    "    if not document_cache:\n",
    "        return \"No documents in the collection.\"\n",
    "    \n",
    "    total_docs = len(document_cache)\n",
    "    total_size = sum(doc[\"size\"] for doc in document_cache.values())\n",
    "    total_words = sum(len(doc[\"content\"].split()) for doc in document_cache.values())\n",
    "    \n",
    "    return f\"\"\"Document Collection Statistics:\n",
    "{'=' * 40}\n",
    "Total Documents: {total_docs}\n",
    "Total Size: {total_size:,} bytes\n",
    "Average Size: {total_size // total_docs:,} bytes\n",
    "Total Words: {total_words:,}\n",
    "Average Words per Document: {total_words // total_docs:,}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "'''\n",
    "\n",
    "with open(\"document_assistant/server_step2.py\", \"w\") as f:\n",
    "    f.write(step2_code)\n",
    "\n",
    "print(\"Step 2 complete: Search tools added\")\n",
    "print(\"\\nNew tools:\")\n",
    "print(\"- list_documents(): List all available documents\")\n",
    "print(\"- search_documents(query): Search by content\")\n",
    "print(\"- get_document_stats(): Get collection statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add Resources\n",
    "\n",
    "Add resources to expose document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_code = '''\n",
    "\"\"\"Document Assistant MCP Server - Step 3: Resources\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_DIR = Path(__file__).parent / \"documents\"\n",
    "\n",
    "# Create the MCP server\n",
    "mcp = FastMCP(\n",
    "    \"Document Assistant\",\n",
    "    description=\"An MCP server for document management and analysis\"\n",
    ")\n",
    "\n",
    "# Document storage\n",
    "document_cache = {}\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"Load all documents from the documents directory.\"\"\"\n",
    "    global document_cache\n",
    "    document_cache = {}\n",
    "    \n",
    "    if not DOCUMENTS_DIR.exists():\n",
    "        return\n",
    "    \n",
    "    for filepath in DOCUMENTS_DIR.glob(\"*.md\"):\n",
    "        doc_id = filepath.stem\n",
    "        stat = filepath.stat()\n",
    "        \n",
    "        document_cache[doc_id] = {\n",
    "            \"id\": doc_id,\n",
    "            \"filename\": filepath.name,\n",
    "            \"path\": str(filepath),\n",
    "            \"content\": filepath.read_text(),\n",
    "            \"size\": stat.st_size,\n",
    "            \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
    "        }\n",
    "\n",
    "load_documents()\n",
    "\n",
    "# ============ TOOLS ============\n",
    "\n",
    "@mcp.tool()\n",
    "def list_documents() -> str:\n",
    "    \"\"\"List all available documents with their metadata.\"\"\"\n",
    "    if not document_cache:\n",
    "        return \"No documents found.\"\n",
    "    \n",
    "    lines = [\"Available Documents:\", \"=\" * 50]\n",
    "    for doc_id, doc in sorted(document_cache.items()):\n",
    "        lines.append(f\"\\nID: {doc_id}\")\n",
    "        lines.append(f\"  Filename: {doc['filename']}\")\n",
    "        lines.append(f\"  Size: {doc['size']} bytes\")\n",
    "        lines.append(f\"  Modified: {doc['modified']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search_documents(query: str, case_sensitive: bool = False) -> str:\n",
    "    \"\"\"Search for documents containing the specified text.\"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Error: Please provide a search query.\"\n",
    "    \n",
    "    results = []\n",
    "    flags = 0 if case_sensitive else re.IGNORECASE\n",
    "    \n",
    "    for doc_id, doc in document_cache.items():\n",
    "        content = doc[\"content\"]\n",
    "        matches = list(re.finditer(re.escape(query), content, flags))\n",
    "        \n",
    "        if matches:\n",
    "            first_match = matches[0]\n",
    "            start = max(0, first_match.start() - 50)\n",
    "            end = min(len(content), first_match.end() + 50)\n",
    "            excerpt = content[start:end].replace(\"\\n\", \" \")\n",
    "            \n",
    "            if start > 0:\n",
    "                excerpt = \"...\" + excerpt\n",
    "            if end < len(content):\n",
    "                excerpt = excerpt + \"...\"\n",
    "            \n",
    "            results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": doc[\"filename\"],\n",
    "                \"match_count\": len(matches),\n",
    "                \"excerpt\": excerpt\n",
    "            })\n",
    "    \n",
    "    if not results:\n",
    "        return f\"No documents found containing '{query}'.\"\n",
    "    \n",
    "    lines = [f\"Found {len(results)} document(s) matching '{query}':\", \"=\" * 50]\n",
    "    for result in results:\n",
    "        lines.append(f\"\\nDocument: {result['doc_id']}\")\n",
    "        lines.append(f\"  Matches: {result['match_count']}\")\n",
    "        lines.append(f\"  Excerpt: {result['excerpt']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_document_stats() -> str:\n",
    "    \"\"\"Get statistics about the document collection.\"\"\"\n",
    "    if not document_cache:\n",
    "        return \"No documents in the collection.\"\n",
    "    \n",
    "    total_docs = len(document_cache)\n",
    "    total_size = sum(doc[\"size\"] for doc in document_cache.values())\n",
    "    total_words = sum(len(doc[\"content\"].split()) for doc in document_cache.values())\n",
    "    \n",
    "    return f\"\"\"Document Collection Statistics:\n",
    "{'=' * 40}\n",
    "Total Documents: {total_docs}\n",
    "Total Size: {total_size:,} bytes\n",
    "Average Size: {total_size // total_docs:,} bytes\n",
    "Total Words: {total_words:,}\n",
    "Average Words per Document: {total_words // total_docs:,}\n",
    "\"\"\"\n",
    "\n",
    "# ============ RESOURCES ============\n",
    "\n",
    "@mcp.resource(\"doc://{doc_id}\")\n",
    "def get_document(doc_id: str) -> str:\n",
    "    \"\"\"Get the full content of a specific document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document identifier (filename without extension)\n",
    "    \n",
    "    Returns:\n",
    "        The full content of the document\n",
    "    \"\"\"\n",
    "    if doc_id not in document_cache:\n",
    "        return f\"Error: Document '{doc_id}' not found.\"\n",
    "    \n",
    "    return document_cache[doc_id][\"content\"]\n",
    "\n",
    "\n",
    "@mcp.resource(\"docs://catalog\")\n",
    "def get_catalog() -> str:\n",
    "    \"\"\"Get the full document catalog as JSON.\n",
    "    \n",
    "    Returns:\n",
    "        JSON-formatted catalog with all document metadata\n",
    "    \"\"\"\n",
    "    catalog = []\n",
    "    for doc_id, doc in document_cache.items():\n",
    "        catalog.append({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"filename\": doc[\"filename\"],\n",
    "            \"size\": doc[\"size\"],\n",
    "            \"modified\": doc[\"modified\"],\n",
    "            \"word_count\": len(doc[\"content\"].split())\n",
    "        })\n",
    "    \n",
    "    return json.dumps(catalog, indent=2)\n",
    "\n",
    "\n",
    "@mcp.resource(\"doc://{doc_id}/metadata\")\n",
    "def get_document_metadata(doc_id: str) -> str:\n",
    "    \"\"\"Get metadata for a specific document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document identifier\n",
    "    \n",
    "    Returns:\n",
    "        JSON-formatted metadata for the document\n",
    "    \"\"\"\n",
    "    if doc_id not in document_cache:\n",
    "        return json.dumps({\"error\": f\"Document '{doc_id}' not found.\"})\n",
    "    \n",
    "    doc = document_cache[doc_id]\n",
    "    metadata = {\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"filename\": doc[\"filename\"],\n",
    "        \"size\": doc[\"size\"],\n",
    "        \"modified\": doc[\"modified\"],\n",
    "        \"word_count\": len(doc[\"content\"].split()),\n",
    "        \"line_count\": doc[\"content\"].count(\"\\n\") + 1\n",
    "    }\n",
    "    \n",
    "    return json.dumps(metadata, indent=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "'''\n",
    "\n",
    "with open(\"document_assistant/server_step3.py\", \"w\") as f:\n",
    "    f.write(step3_code)\n",
    "\n",
    "print(\"Step 3 complete: Resources added\")\n",
    "print(\"\\nNew resources:\")\n",
    "print(\"- doc://{doc_id} - Full document content\")\n",
    "print(\"- docs://catalog - Document catalog (JSON)\")\n",
    "print(\"- doc://{doc_id}/metadata - Document metadata (JSON)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add Prompts\n",
    "\n",
    "Add prompts for common document analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4_code = '''\n",
    "\"\"\"Document Assistant MCP Server - Step 4: Prompts (Complete Server)\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Configuration\n",
    "DOCUMENTS_DIR = Path(__file__).parent / \"documents\"\n",
    "\n",
    "# Create the MCP server\n",
    "mcp = FastMCP(\n",
    "    \"Document Assistant\",\n",
    "    description=\"An MCP server for document management and analysis\"\n",
    ")\n",
    "\n",
    "# Document storage\n",
    "document_cache = {}\n",
    "\n",
    "def load_documents():\n",
    "    \"\"\"Load all documents from the documents directory.\"\"\"\n",
    "    global document_cache\n",
    "    document_cache = {}\n",
    "    \n",
    "    if not DOCUMENTS_DIR.exists():\n",
    "        return\n",
    "    \n",
    "    for filepath in DOCUMENTS_DIR.glob(\"*.md\"):\n",
    "        doc_id = filepath.stem\n",
    "        stat = filepath.stat()\n",
    "        \n",
    "        document_cache[doc_id] = {\n",
    "            \"id\": doc_id,\n",
    "            \"filename\": filepath.name,\n",
    "            \"path\": str(filepath),\n",
    "            \"content\": filepath.read_text(),\n",
    "            \"size\": stat.st_size,\n",
    "            \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
    "        }\n",
    "\n",
    "load_documents()\n",
    "\n",
    "# ============ TOOLS ============\n",
    "\n",
    "@mcp.tool()\n",
    "def list_documents() -> str:\n",
    "    \"\"\"List all available documents with their metadata.\"\"\"\n",
    "    if not document_cache:\n",
    "        return \"No documents found.\"\n",
    "    \n",
    "    lines = [\"Available Documents:\", \"=\" * 50]\n",
    "    for doc_id, doc in sorted(document_cache.items()):\n",
    "        lines.append(f\"\\nID: {doc_id}\")\n",
    "        lines.append(f\"  Filename: {doc['filename']}\")\n",
    "        lines.append(f\"  Size: {doc['size']} bytes\")\n",
    "        lines.append(f\"  Modified: {doc['modified']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def search_documents(query: str, case_sensitive: bool = False) -> str:\n",
    "    \"\"\"Search for documents containing the specified text.\"\"\"\n",
    "    if not query.strip():\n",
    "        return \"Error: Please provide a search query.\"\n",
    "    \n",
    "    results = []\n",
    "    flags = 0 if case_sensitive else re.IGNORECASE\n",
    "    \n",
    "    for doc_id, doc in document_cache.items():\n",
    "        content = doc[\"content\"]\n",
    "        matches = list(re.finditer(re.escape(query), content, flags))\n",
    "        \n",
    "        if matches:\n",
    "            first_match = matches[0]\n",
    "            start = max(0, first_match.start() - 50)\n",
    "            end = min(len(content), first_match.end() + 50)\n",
    "            excerpt = content[start:end].replace(\"\\n\", \" \")\n",
    "            \n",
    "            if start > 0:\n",
    "                excerpt = \"...\" + excerpt\n",
    "            if end < len(content):\n",
    "                excerpt = excerpt + \"...\"\n",
    "            \n",
    "            results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"filename\": doc[\"filename\"],\n",
    "                \"match_count\": len(matches),\n",
    "                \"excerpt\": excerpt\n",
    "            })\n",
    "    \n",
    "    if not results:\n",
    "        return f\"No documents found containing '{query}'.\"\n",
    "    \n",
    "    lines = [f\"Found {len(results)} document(s) matching '{query}':\", \"=\" * 50]\n",
    "    for result in results:\n",
    "        lines.append(f\"\\nDocument: {result['doc_id']}\")\n",
    "        lines.append(f\"  Matches: {result['match_count']}\")\n",
    "        lines.append(f\"  Excerpt: {result['excerpt']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def get_document_stats() -> str:\n",
    "    \"\"\"Get statistics about the document collection.\"\"\"\n",
    "    if not document_cache:\n",
    "        return \"No documents in the collection.\"\n",
    "    \n",
    "    total_docs = len(document_cache)\n",
    "    total_size = sum(doc[\"size\"] for doc in document_cache.values())\n",
    "    total_words = sum(len(doc[\"content\"].split()) for doc in document_cache.values())\n",
    "    \n",
    "    return f\"\"\"Document Collection Statistics:\n",
    "{'=' * 40}\n",
    "Total Documents: {total_docs}\n",
    "Total Size: {total_size:,} bytes\n",
    "Average Size: {total_size // total_docs:,} bytes\n",
    "Total Words: {total_words:,}\n",
    "Average Words per Document: {total_words // total_docs:,}\n",
    "\"\"\"\n",
    "\n",
    "# ============ RESOURCES ============\n",
    "\n",
    "@mcp.resource(\"doc://{doc_id}\")\n",
    "def get_document(doc_id: str) -> str:\n",
    "    \"\"\"Get the full content of a specific document.\"\"\"\n",
    "    if doc_id not in document_cache:\n",
    "        return f\"Error: Document '{doc_id}' not found.\"\n",
    "    return document_cache[doc_id][\"content\"]\n",
    "\n",
    "\n",
    "@mcp.resource(\"docs://catalog\")\n",
    "def get_catalog() -> str:\n",
    "    \"\"\"Get the full document catalog as JSON.\"\"\"\n",
    "    catalog = []\n",
    "    for doc_id, doc in document_cache.items():\n",
    "        catalog.append({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"filename\": doc[\"filename\"],\n",
    "            \"size\": doc[\"size\"],\n",
    "            \"modified\": doc[\"modified\"],\n",
    "            \"word_count\": len(doc[\"content\"].split())\n",
    "        })\n",
    "    return json.dumps(catalog, indent=2)\n",
    "\n",
    "\n",
    "@mcp.resource(\"doc://{doc_id}/metadata\")\n",
    "def get_document_metadata(doc_id: str) -> str:\n",
    "    \"\"\"Get metadata for a specific document.\"\"\"\n",
    "    if doc_id not in document_cache:\n",
    "        return json.dumps({\"error\": f\"Document '{doc_id}' not found.\"})\n",
    "    \n",
    "    doc = document_cache[doc_id]\n",
    "    metadata = {\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"filename\": doc[\"filename\"],\n",
    "        \"size\": doc[\"size\"],\n",
    "        \"modified\": doc[\"modified\"],\n",
    "        \"word_count\": len(doc[\"content\"].split()),\n",
    "        \"line_count\": doc[\"content\"].count(\"\\n\") + 1\n",
    "    }\n",
    "    return json.dumps(metadata, indent=2)\n",
    "\n",
    "# ============ PROMPTS ============\n",
    "\n",
    "@mcp.prompt()\n",
    "def summarize(doc_id: str) -> str:\n",
    "    \"\"\"Create a prompt to summarize a document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document to summarize\n",
    "    \"\"\"\n",
    "    if doc_id not in document_cache:\n",
    "        return f\"Error: Document '{doc_id}' not found.\"\n",
    "    \n",
    "    doc = document_cache[doc_id]\n",
    "    return f\"\"\"Please summarize the following document:\n",
    "\n",
    "Document: {doc['filename']}\n",
    "---\n",
    "{doc['content']}\n",
    "---\n",
    "\n",
    "Provide:\n",
    "1. A brief summary (2-3 sentences)\n",
    "2. Key points (bullet list)\n",
    "3. Any action items or next steps mentioned\"\"\"\n",
    "\n",
    "\n",
    "@mcp.prompt()\n",
    "def compare(doc_id_1: str, doc_id_2: str) -> str:\n",
    "    \"\"\"Create a prompt to compare two documents.\n",
    "    \n",
    "    Args:\n",
    "        doc_id_1: The first document to compare\n",
    "        doc_id_2: The second document to compare\n",
    "    \"\"\"\n",
    "    if doc_id_1 not in document_cache:\n",
    "        return f\"Error: Document '{doc_id_1}' not found.\"\n",
    "    if doc_id_2 not in document_cache:\n",
    "        return f\"Error: Document '{doc_id_2}' not found.\"\n",
    "    \n",
    "    doc1 = document_cache[doc_id_1]\n",
    "    doc2 = document_cache[doc_id_2]\n",
    "    \n",
    "    return f\"\"\"Please compare the following two documents:\n",
    "\n",
    "=== Document 1: {doc1['filename']} ===\n",
    "{doc1['content']}\n",
    "\n",
    "=== Document 2: {doc2['filename']} ===\n",
    "{doc2['content']}\n",
    "\n",
    "Provide:\n",
    "1. Main similarities between the documents\n",
    "2. Key differences\n",
    "3. How they might relate to each other\n",
    "4. Any contradictions or inconsistencies\"\"\"\n",
    "\n",
    "\n",
    "@mcp.prompt()\n",
    "def extract_insights(doc_id: str, focus_area: str = \"general\") -> str:\n",
    "    \"\"\"Create a prompt to extract insights from a document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document to analyze\n",
    "        focus_area: The area to focus on (e.g., 'technical', 'business', 'security', 'general')\n",
    "    \"\"\"\n",
    "    if doc_id not in document_cache:\n",
    "        return f\"Error: Document '{doc_id}' not found.\"\n",
    "    \n",
    "    doc = document_cache[doc_id]\n",
    "    \n",
    "    focus_instructions = {\n",
    "        \"general\": \"Extract any important insights, patterns, or notable information.\",\n",
    "        \"technical\": \"Focus on technical details, architecture decisions, and implementation specifics.\",\n",
    "        \"business\": \"Focus on business impact, ROI, timelines, and strategic implications.\",\n",
    "        \"security\": \"Focus on security considerations, risks, compliance requirements, and vulnerabilities.\"\n",
    "    }\n",
    "    \n",
    "    instruction = focus_instructions.get(focus_area, focus_instructions[\"general\"])\n",
    "    \n",
    "    return f\"\"\"Please analyze the following document and extract key insights:\n",
    "\n",
    "Document: {doc['filename']}\n",
    "Focus Area: {focus_area}\n",
    "---\n",
    "{doc['content']}\n",
    "---\n",
    "\n",
    "{instruction}\n",
    "\n",
    "Format your response as:\n",
    "1. Key Insights (numbered list)\n",
    "2. Recommendations based on the document\n",
    "3. Questions that should be addressed\n",
    "4. Related topics to explore\"\"\"\n",
    "\n",
    "\n",
    "@mcp.prompt()\n",
    "def generate_questions(doc_id: str) -> str:\n",
    "    \"\"\"Create a prompt to generate questions about a document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document to generate questions for\n",
    "    \"\"\"\n",
    "    if doc_id not in document_cache:\n",
    "        return f\"Error: Document '{doc_id}' not found.\"\n",
    "    \n",
    "    doc = document_cache[doc_id]\n",
    "    \n",
    "    return f\"\"\"Based on the following document, generate thoughtful questions that would help clarify or expand on the content:\n",
    "\n",
    "Document: {doc['filename']}\n",
    "---\n",
    "{doc['content']}\n",
    "---\n",
    "\n",
    "Generate:\n",
    "1. 5 clarifying questions (things that are unclear or need more detail)\n",
    "2. 5 follow-up questions (things to explore further)\n",
    "3. 3 devil's advocate questions (potential issues or challenges)\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "'''\n",
    "\n",
    "# Save as the final server\n",
    "with open(\"document_assistant/server.py\", \"w\") as f:\n",
    "    f.write(step4_code)\n",
    "\n",
    "print(\"Step 4 complete: Prompts added - Server is complete!\")\n",
    "print(\"\\nNew prompts:\")\n",
    "print(\"- summarize(doc_id) - Summarize a document\")\n",
    "print(\"- compare(doc_id_1, doc_id_2) - Compare two documents\")\n",
    "print(\"- extract_insights(doc_id, focus_area) - Extract insights with focus\")\n",
    "print(\"- generate_questions(doc_id) - Generate questions about a document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing Your Server\n",
    "\n",
    "Let's test the complete Document Assistant server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with MCP Inspector\n",
    "\n",
    "Run in terminal:\n",
    "```bash\n",
    "cd document_assistant\n",
    "mcp dev server.py\n",
    "```\n",
    "\n",
    "Then test:\n",
    "1. **Tools**: Call `list_documents()`, `search_documents(\"AI\")`, `get_document_stats()`\n",
    "2. **Resources**: Access `doc://project_proposal`, `docs://catalog`\n",
    "3. **Prompts**: Use `summarize` with `doc_id=\"project_proposal\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test checklist\n",
    "\n",
    "test_results = {\n",
    "    \"tools\": {\n",
    "        \"list_documents\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        },\n",
    "        \"search_documents\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"query_used\": \"\",\n",
    "            \"results_count\": 0,\n",
    "            \"notes\": \"\"\n",
    "        },\n",
    "        \"get_document_stats\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        }\n",
    "    },\n",
    "    \"resources\": {\n",
    "        \"doc://project_proposal\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        },\n",
    "        \"docs://catalog\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        },\n",
    "        \"doc://project_proposal/metadata\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        }\n",
    "    },\n",
    "    \"prompts\": {\n",
    "        \"summarize\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        },\n",
    "        \"compare\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        },\n",
    "        \"extract_insights\": {\n",
    "            \"tested\": False,\n",
    "            \"passed\": False,\n",
    "            \"notes\": \"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Test your server with MCP Inspector and fill in the results above!\")\n",
    "print(\"\\nRun: cd document_assistant && mcp dev server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: VS Code Integration\n",
    "\n",
    "Connect your Document Assistant to VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the full path to the server\n",
    "server_path = os.path.abspath(\"document_assistant/server.py\")\n",
    "\n",
    "print(\"Add this to your VS Code settings.json:\")\n",
    "print()\n",
    "print('''{\n",
    "  \"github.copilot.chat.experimental.mcp.servers\": {\n",
    "    \"document-assistant\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"''' + server_path + '''\"]\n",
    "    }\n",
    "  }\n",
    "}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing in VS Code\n",
    "\n",
    "After configuring, try these prompts in GitHub Copilot Chat:\n",
    "\n",
    "1. \"List all my documents\"\n",
    "2. \"Search for documents about AI\"\n",
    "3. \"Summarize the project proposal\"\n",
    "4. \"Compare the project proposal with the technical spec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document your VS Code testing\n",
    "\n",
    "vscode_tests = {\n",
    "    \"server_connected\": False,\n",
    "    \"test_prompts\": [\n",
    "        {\n",
    "            \"prompt\": \"List all my documents\",\n",
    "            \"tool_called\": \"\",\n",
    "            \"result_quality\": \"\"  # Good/Okay/Poor\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Search for documents about AI\",\n",
    "            \"tool_called\": \"\",\n",
    "            \"result_quality\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Summarize the project proposal\",\n",
    "            \"tool_called\": \"\",\n",
    "            \"result_quality\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"overall_experience\": \"\"\n",
    "}\n",
    "\n",
    "print(\"Document your VS Code integration experience above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Extension Challenges\n",
    "\n",
    "Extend your Document Assistant with additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Add Document Creation\n",
    "\n",
    "Add a tool to create new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a create_document tool\n",
    "\n",
    "create_document_code = '''\n",
    "@mcp.tool()\n",
    "def create_document(doc_id: str, content: str, overwrite: bool = False) -> str:\n",
    "    \"\"\"Create a new document in the collection.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The identifier for the new document (will be used as filename)\n",
    "        content: The content of the document\n",
    "        overwrite: Whether to overwrite if document exists (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        Success message or error\n",
    "    \"\"\"\n",
    "    # TODO: Implement this tool\n",
    "    # 1. Check if document already exists (unless overwrite=True)\n",
    "    # 2. Write the file to DOCUMENTS_DIR\n",
    "    # 3. Reload the document cache\n",
    "    # 4. Return success message\n",
    "    pass\n",
    "'''\n",
    "\n",
    "print(\"Challenge 1: Implement the create_document tool\")\n",
    "print(create_document_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Add Tag Support\n",
    "\n",
    "Add tagging capabilities to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement tag support\n",
    "\n",
    "tag_support_code = '''\n",
    "# Store tags in a separate JSON file\n",
    "TAGS_FILE = DOCUMENTS_DIR / \"tags.json\"\n",
    "\n",
    "def load_tags() -> dict:\n",
    "    \"\"\"Load tags from the tags file.\"\"\"\n",
    "    if TAGS_FILE.exists():\n",
    "        return json.loads(TAGS_FILE.read_text())\n",
    "    return {}\n",
    "\n",
    "def save_tags(tags: dict):\n",
    "    \"\"\"Save tags to the tags file.\"\"\"\n",
    "    TAGS_FILE.write_text(json.dumps(tags, indent=2))\n",
    "\n",
    "@mcp.tool()\n",
    "def add_tag(doc_id: str, tag: str) -> str:\n",
    "    \"\"\"Add a tag to a document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document to tag\n",
    "        tag: The tag to add\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "@mcp.tool()\n",
    "def search_by_tag(tag: str) -> str:\n",
    "    \"\"\"Find all documents with a specific tag.\n",
    "    \n",
    "    Args:\n",
    "        tag: The tag to search for\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "@mcp.tool()\n",
    "def list_tags() -> str:\n",
    "    \"\"\"List all tags and their document counts.\"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "'''\n",
    "\n",
    "print(\"Challenge 2: Implement tag support\")\n",
    "print(tag_support_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Add Version History\n",
    "\n",
    "Track document versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement version history\n",
    "\n",
    "version_history_code = '''\n",
    "# Store versions in a subdirectory\n",
    "VERSIONS_DIR = DOCUMENTS_DIR / \".versions\"\n",
    "\n",
    "@mcp.tool()\n",
    "def update_document(doc_id: str, new_content: str) -> str:\n",
    "    \"\"\"Update a document, saving the previous version.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document to update\n",
    "        new_content: The new content\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    # 1. Save current version to VERSIONS_DIR with timestamp\n",
    "    # 2. Write new content to document\n",
    "    # 3. Reload cache\n",
    "    pass\n",
    "\n",
    "@mcp.tool()\n",
    "def list_versions(doc_id: str) -> str:\n",
    "    \"\"\"List all versions of a document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document to list versions for\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "@mcp.resource(\"doc://{doc_id}/version/{version_id}\")\n",
    "def get_document_version(doc_id: str, version_id: str) -> str:\n",
    "    \"\"\"Get a specific version of a document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: The document\n",
    "        version_id: The version timestamp\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "'''\n",
    "\n",
    "print(\"Challenge 3: Implement version history\")\n",
    "print(version_history_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Challenge 4: Payments Industry Extension - Compliance Document Assistant\n\nExtend the Document Assistant for a payments/financial services context. Build a Compliance Document Assistant that helps with regulatory documentation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Payments Industry Extension: Compliance Document Assistant\n# This extension adds compliance-specific features for payments industry\n\ncompliance_extension_code = '''\n\"\"\"\nCompliance Document Assistant - Payments Industry Extension\n\nThis extension adds compliance-focused features to the Document Assistant,\nsuitable for financial services and payments companies.\n\nNew capabilities:\n1. Regulatory compliance checking\n2. PCI-DSS requirement mapping\n3. Audit trail for document access\n4. Sensitive data detection\n\"\"\"\n\nimport json\nfrom datetime import datetime\nfrom mcp.server.fastmcp import FastMCP\n\n# Add these features to your existing server.py\n\n# ============ COMPLIANCE DATA ============\n\n# PCI-DSS Requirements mapping\nPCI_DSS_REQUIREMENTS = {\n    \"1\": \"Install and maintain network security controls\",\n    \"2\": \"Apply secure configurations to system components\",\n    \"3\": \"Protect stored account data\",\n    \"4\": \"Protect cardholder data with strong cryptography\",\n    \"5\": \"Protect systems from malicious software\",\n    \"6\": \"Develop and maintain secure systems and software\",\n    \"7\": \"Restrict access to system components\",\n    \"8\": \"Identify users and authenticate access\",\n    \"9\": \"Restrict physical access to cardholder data\",\n    \"10\": \"Log and monitor all access to system components\",\n    \"11\": \"Test security of systems and networks regularly\",\n    \"12\": \"Support information security with organizational policies\"\n}\n\n# Sensitive data patterns (for detection)\nSENSITIVE_PATTERNS = {\n    \"card_number\": r\"\\\\b(?:\\\\d{4}[- ]?){3}\\\\d{4}\\\\b\",\n    \"ssn\": r\"\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\",\n    \"account_number\": r\"\\\\b[A-Z]{2}\\\\d{2}[A-Z0-9]{4}\\\\d{7}([A-Z0-9]?){0,16}\\\\b\",  # IBAN pattern\n    \"cvv\": r\"\\\\bCVV[:\\\\s]*\\\\d{3,4}\\\\b\",\n}\n\n# Audit log storage\naudit_log = []\n\n# ============ COMPLIANCE TOOLS ============\n\n@mcp.tool()\ndef check_pci_compliance(doc_id: str) -> str:\n    \"\"\"Check a document for PCI-DSS compliance coverage.\n    \n    Args:\n        doc_id: The document to analyze for PCI-DSS compliance coverage\n    \n    Returns:\n        Analysis of which PCI-DSS requirements the document addresses\n    \"\"\"\n    if doc_id not in document_cache:\n        return f\"Document {doc_id} not found\"\n    \n    doc = document_cache[doc_id]\n    content = doc[\"content\"].lower()\n    \n    # Keywords associated with each PCI requirement\n    requirement_keywords = {\n        \"1\": [\"firewall\", \"network security\", \"traffic control\", \"dmz\"],\n        \"2\": [\"configuration\", \"default password\", \"vendor defaults\", \"hardening\"],\n        \"3\": [\"encryption\", \"storage\", \"retention\", \"disposal\", \"masking\"],\n        \"4\": [\"transmission\", \"ssl\", \"tls\", \"cryptography\", \"transit\"],\n        \"5\": [\"antivirus\", \"malware\", \"anti-malware\", \"virus\"],\n        \"6\": [\"secure development\", \"code review\", \"patching\", \"vulnerability\"],\n        \"7\": [\"access control\", \"need to know\", \"least privilege\", \"authorization\"],\n        \"8\": [\"authentication\", \"password\", \"mfa\", \"two-factor\", \"identity\"],\n        \"9\": [\"physical security\", \"badge\", \"visitor\", \"media\"],\n        \"10\": [\"logging\", \"audit trail\", \"monitoring\", \"log\"],\n        \"11\": [\"penetration testing\", \"vulnerability scan\", \"security testing\"],\n        \"12\": [\"policy\", \"security awareness\", \"training\", \"incident response\"]\n    }\n    \n    covered = []\n    not_covered = []\n    \n    for req_num, keywords in requirement_keywords.items():\n        found_keywords = [kw for kw in keywords if kw in content]\n        if found_keywords:\n            covered.append({\n                \"requirement\": req_num,\n                \"description\": PCI_DSS_REQUIREMENTS[req_num],\n                \"keywords_found\": found_keywords\n            })\n        else:\n            not_covered.append({\n                \"requirement\": req_num,\n                \"description\": PCI_DSS_REQUIREMENTS[req_num]\n            })\n    \n    # Log the compliance check\n    audit_log.append({\n        \"action\": \"pci_compliance_check\",\n        \"document\": doc_id,\n        \"timestamp\": datetime.now().isoformat(),\n        \"result\": f\"{len(covered)}/{len(PCI_DSS_REQUIREMENTS)} requirements addressed\"\n    })\n    \n    result = f\"\"\"\nPCI-DSS COMPLIANCE ANALYSIS\n===========================\nDocument: {doc_id}\nAnalysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\nREQUIREMENTS ADDRESSED ({len(covered)}/{len(PCI_DSS_REQUIREMENTS)}):\n\"\"\"\n    for item in covered:\n        result += f\"\\\\n✅ Req {item['requirement']}: {item['description']}\"\n        result += f\"\\\\n   Keywords: {', '.join(item['keywords_found'])}\"\n    \n    result += f\"\\\\n\\\\nREQUIREMENTS NOT ADDRESSED ({len(not_covered)}):\"\n    for item in not_covered:\n        result += f\"\\\\n❌ Req {item['requirement']}: {item['description']}\"\n    \n    result += f\"\\\\n\\\\nCOMPLIANCE SCORE: {len(covered) * 100 // len(PCI_DSS_REQUIREMENTS)}%\"\n    \n    return result\n\n\n@mcp.tool()\ndef scan_sensitive_data(doc_id: str) -> str:\n    \"\"\"Scan a document for potentially sensitive data patterns.\n    \n    Args:\n        doc_id: The document to scan for sensitive data\n    \n    Returns:\n        Report of any sensitive data patterns found\n    \"\"\"\n    import re\n    \n    if doc_id not in document_cache:\n        return f\"Document {doc_id} not found\"\n    \n    doc = document_cache[doc_id]\n    content = doc[\"content\"]\n    \n    findings = []\n    \n    for data_type, pattern in SENSITIVE_PATTERNS.items():\n        matches = re.findall(pattern, content, re.IGNORECASE)\n        if matches:\n            # Mask the actual data in the report\n            masked = [m[:4] + \"*\" * (len(m) - 8) + m[-4:] if len(m) > 8 else \"****\" for m in matches]\n            findings.append({\n                \"type\": data_type,\n                \"count\": len(matches),\n                \"masked_samples\": masked[:3]  # Only show first 3\n            })\n    \n    # Log the scan\n    audit_log.append({\n        \"action\": \"sensitive_data_scan\",\n        \"document\": doc_id,\n        \"timestamp\": datetime.now().isoformat(),\n        \"findings_count\": len(findings)\n    })\n    \n    if not findings:\n        return f\"✅ No sensitive data patterns detected in {doc_id}\"\n    \n    result = f\"\"\"\n⚠️ SENSITIVE DATA SCAN RESULTS\n==============================\nDocument: {doc_id}\nScan Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\nFINDINGS:\n\"\"\"\n    for finding in findings:\n        result += f\"\\\\n🔴 {finding['type'].upper()}: {finding['count']} occurrence(s)\"\n        result += f\"\\\\n   Samples (masked): {finding['masked_samples']}\"\n    \n    result += \"\\\\n\\\\n⚠️ RECOMMENDATION: Review and redact sensitive data before sharing.\"\n    \n    return result\n\n\n@mcp.tool()\ndef get_audit_log(limit: int = 10) -> str:\n    \"\"\"Retrieve recent audit log entries.\n    \n    Args:\n        limit: Maximum number of entries to return (default: 10)\n    \n    Returns:\n        Recent audit log entries\n    \"\"\"\n    recent = audit_log[-limit:] if audit_log else []\n    \n    if not recent:\n        return \"No audit log entries found.\"\n    \n    result = f\"AUDIT LOG (Last {len(recent)} entries)\\\\n\" + \"=\" * 40\n    \n    for entry in reversed(recent):\n        result += f\"\\\\n\\\\n[{entry['timestamp']}]\"\n        result += f\"\\\\n  Action: {entry['action']}\"\n        result += f\"\\\\n  Document: {entry['document']}\"\n        if 'result' in entry:\n            result += f\"\\\\n  Result: {entry['result']}\"\n    \n    return result\n\n\n# ============ COMPLIANCE RESOURCES ============\n\n@mcp.resource(\"compliance://pci-dss\")\ndef get_pci_requirements() -> str:\n    \"\"\"Get the full list of PCI-DSS requirements.\"\"\"\n    result = \"PCI-DSS v4.0 Requirements Summary\\\\n\" + \"=\" * 40 + \"\\\\n\"\n    for req_num, desc in PCI_DSS_REQUIREMENTS.items():\n        result += f\"\\\\nRequirement {req_num}: {desc}\"\n    return result\n\n\n@mcp.resource(\"compliance://audit-log\")\ndef get_full_audit_log() -> str:\n    \"\"\"Get the complete audit log as JSON.\"\"\"\n    return json.dumps(audit_log, indent=2)\n\n\n# ============ COMPLIANCE PROMPTS ============\n\n@mcp.prompt()\ndef compliance_review(doc_id: str) -> str:\n    \"\"\"Create a prompt for comprehensive compliance review.\n    \n    Args:\n        doc_id: The document to review for compliance\n    \"\"\"\n    if doc_id not in document_cache:\n        return f\"Document {doc_id} not found\"\n    \n    doc = document_cache[doc_id]\n    \n    return f\"\"\"Please perform a comprehensive compliance review of this document:\n\nDocument: {doc['filename']}\n---\n{doc['content']}\n---\n\nReview for:\n1. **Data Privacy**: Are there any PII or sensitive data handling concerns?\n2. **PCI-DSS Alignment**: Which PCI requirements does this document address?\n3. **Policy Gaps**: What security policies might be missing?\n4. **Regulatory Compliance**: Any SOX, GDPR, or other regulatory considerations?\n5. **Risk Assessment**: What risks does this document mitigate or introduce?\n\nProvide a structured compliance report with:\n- Compliance Score (1-10)\n- Key Findings\n- Required Actions\n- Recommended Improvements\"\"\"\n\n\n@mcp.prompt()\ndef generate_policy(policy_type: str) -> str:\n    \"\"\"Create a prompt to generate a compliance policy.\n    \n    Args:\n        policy_type: Type of policy (e.g., 'data_retention', 'access_control', 'incident_response')\n    \"\"\"\n    policy_templates = {\n        \"data_retention\": \"Data Retention and Disposal Policy for payment card data\",\n        \"access_control\": \"Access Control Policy for systems handling cardholder data\",\n        \"incident_response\": \"Security Incident Response Plan for data breaches\",\n        \"encryption\": \"Encryption and Key Management Policy\",\n        \"vendor_management\": \"Third-Party Vendor Security Assessment Policy\"\n    }\n    \n    policy_desc = policy_templates.get(policy_type, f\"Custom policy: {policy_type}\")\n    \n    return f\"\"\"Generate a comprehensive {policy_desc} for a payments company.\n\nThe policy should include:\n1. **Purpose and Scope**\n2. **Policy Statement**\n3. **Roles and Responsibilities**\n4. **Procedures**\n5. **Compliance Requirements** (reference PCI-DSS where applicable)\n6. **Exceptions Process**\n7. **Enforcement**\n8. **Review Schedule**\n\nMake the policy:\n- Clear and actionable\n- Aligned with PCI-DSS v4.0 requirements\n- Suitable for a financial services organization\n- Ready for review by compliance team\"\"\"\n'''\n\nprint(\"Compliance Document Assistant Extension\")\nprint(\"=\" * 50)\nprint(\"\\\\nNEW TOOLS:\")\nprint(\"  - check_pci_compliance(doc_id) - Analyze PCI-DSS coverage\")\nprint(\"  - scan_sensitive_data(doc_id) - Detect sensitive data patterns\")\nprint(\"  - get_audit_log(limit) - View document access audit trail\")\nprint(\"\\\\nNEW RESOURCES:\")\nprint(\"  - compliance://pci-dss - PCI-DSS requirements reference\")\nprint(\"  - compliance://audit-log - Full audit log (JSON)\")\nprint(\"\\\\nNEW PROMPTS:\")\nprint(\"  - compliance_review(doc_id) - Comprehensive compliance review\")\nprint(\"  - generate_policy(policy_type) - Generate compliance policies\")\nprint(\"\\\\nTo implement: Add this code to your server.py file\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Programmatic Access\n",
    "\n",
    "Use the MCP Client SDK to access your Document Assistant programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic client for Document Assistant\n",
    "\n",
    "client_code = '''\n",
    "\"\"\"Programmatic client for the Document Assistant MCP server.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "async def main():\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[\"server.py\"]\n",
    "    )\n",
    "    \n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize\n",
    "            await session.initialize()\n",
    "            \n",
    "            # List all documents\n",
    "            print(\"=\" * 50)\n",
    "            print(\"Listing documents...\")\n",
    "            result = await session.call_tool(\"list_documents\", {})\n",
    "            print(result.content[0].text)\n",
    "            \n",
    "            # Search for \"AI\"\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"Searching for 'AI'...\")\n",
    "            result = await session.call_tool(\"search_documents\", {\"query\": \"AI\"})\n",
    "            print(result.content[0].text)\n",
    "            \n",
    "            # Get document via resource\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"Reading project_proposal via resource...\")\n",
    "            resources = await session.list_resources()\n",
    "            for resource in resources.resources:\n",
    "                print(f\"  Available: {resource.uri}\")\n",
    "            \n",
    "            # Get collection stats\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"Getting statistics...\")\n",
    "            result = await session.call_tool(\"get_document_stats\", {})\n",
    "            print(result.content[0].text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "\n",
    "with open(\"document_assistant/client.py\", \"w\") as f:\n",
    "    f.write(client_code)\n",
    "\n",
    "print(\"Client code saved to document_assistant/client.py\")\n",
    "print(\"\\nRun with: cd document_assistant && python client.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Final Review\n",
    "\n",
    "Let's review what you've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final project summary\n",
    "\n",
    "project_summary = {\n",
    "    \"server_name\": \"Document Assistant\",\n",
    "    \"components\": {\n",
    "        \"tools\": [\n",
    "            \"list_documents() - List all documents\",\n",
    "            \"search_documents(query) - Search by content\",\n",
    "            \"get_document_stats() - Collection statistics\"\n",
    "        ],\n",
    "        \"resources\": [\n",
    "            \"doc://{doc_id} - Document content\",\n",
    "            \"docs://catalog - Full catalog (JSON)\",\n",
    "            \"doc://{doc_id}/metadata - Document metadata\"\n",
    "        ],\n",
    "        \"prompts\": [\n",
    "            \"summarize(doc_id) - Summarize document\",\n",
    "            \"compare(doc_id_1, doc_id_2) - Compare documents\",\n",
    "            \"extract_insights(doc_id, focus_area) - Extract insights\",\n",
    "            \"generate_questions(doc_id) - Generate questions\"\n",
    "        ]\n",
    "    },\n",
    "    \"files_created\": [\n",
    "        \"document_assistant/server.py - Main server\",\n",
    "        \"document_assistant/client.py - Programmatic client\",\n",
    "        \"document_assistant/documents/*.md - Sample documents\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Document Assistant MCP Server\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTools:\")\n",
    "for tool in project_summary[\"components\"][\"tools\"]:\n",
    "    print(f\"  - {tool}\")\n",
    "print(\"\\nResources:\")\n",
    "for resource in project_summary[\"components\"][\"resources\"]:\n",
    "    print(f\"  - {resource}\")\n",
    "print(\"\\nPrompts:\")\n",
    "for prompt in project_summary[\"components\"][\"prompts\"]:\n",
    "    print(f\"  - {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "Congratulations! You've built a complete Document Assistant MCP server that demonstrates:\n",
    "\n",
    "1. **Tools** for searching and listing documents\n",
    "2. **Resources** for accessing document contents and metadata\n",
    "3. **Prompts** for common document analysis tasks\n",
    "4. **Integration** with VS Code and programmatic access\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Tools** are for actions (search, list, stats)\n",
    "- **Resources** are for data access (read content, get metadata)\n",
    "- **Prompts** are for reusable workflows (summarize, compare, analyze)\n",
    "- Good docstrings are critical for tool discovery\n",
    "- Test with MCP Inspector before integrating\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- Add your own document types and tools\n",
    "- Implement the extension challenges\n",
    "- Connect to real document storage (S3, database, etc.)\n",
    "- Build more complex analysis prompts\n",
    "\n",
    "### Workshop Complete!\n",
    "\n",
    "You now have the skills to build custom MCP servers for any use case. The patterns you learned apply to:\n",
    "- Database assistants\n",
    "- API integrations\n",
    "- Development tools\n",
    "- Business applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final reflection\n",
    "\n",
    "reflection = {\n",
    "    \"most_valuable_learning\": \"\",\n",
    "    \"challenges_faced\": \"\",\n",
    "    \"ideas_for_own_projects\": \"\",\n",
    "    \"questions_remaining\": \"\"\n",
    "}\n",
    "\n",
    "print(\"Reflect on your learning experience above!\")\n",
    "print(\"\\nThank you for completing the MCP Workshop!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}